{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics in Python\n",
    "\n",
    "[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.14957301.svg)](https://doi.org/10.5281/zenodo.14957301)\n",
    "\n",
    "**This notebook is for the SP24 Public Sector Data Sciences and Management class at John Glenn College of Public Affairs.**\n",
    "\n",
    "**Author: Tian Lou**\n",
    "\n",
    "The Ohio State University\n",
    "\n",
    "**Citation**: *Lou, T. (2025, March 2). Public Sector Data Science Class: Basics in Python. Zenodo. https://doi.org/10.5281/zenodo.14957301*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "In this notebook, I will use publicly available data to show you the basic functions we commonly use for data analysis in Python. After walking through this notebook, you should:\n",
    "- Gain a basic understanding of the Pandas and Numpy libraries in Python\n",
    "- Know how to import data in Python\n",
    "- Learn how to obtain basic information of a DataFrame, such as number of rows and number of columns\n",
    "- Understand how to slice a DataFrame and how to generate new columns based on existing columns in a DataFrame\n",
    "- Be able to generate simple descriptive statistics and simple visualizations\n",
    "\n",
    "*If this is the first time (or one of the first times) you are using Python and Jupyter Notebook, you may feel a bit overwhelmed or experience a sense of excitement and freshness. (I hope it's the latter!) Either way, please take it easy. Follow the guidance in this notebook to explore various Python functions. Experiment with different coding approaches. I'm confident that you will have fun and pick up a few tricks during this process! :)*\n",
    "\n",
    "## Data\n",
    "The data used in this notebook is [DOL ETA 203 report (characteristics of insured unemployed).](https://oui.doleta.gov/unemploy/DataDownloads.asp) Below is DOL's description of the data: \n",
    "\n",
    "    \"The ETA 203 report provides information, by state and for the Nation, about the characteristics of Unemployment Insurance (UI) claimants. This data is useful in describing the population of claimants and determining how that population changes over time and under various conditions. It can also be compared with characteristic figures of the total unemployed as published by the Bureau of Labor Statistics (BLS).\"\n",
    "    \n",
    "The data dictionary of ETA 203 report can be found [here](https://oui.doleta.gov/dmstree/handbooks/402/402_4/4024c6/4024c6.pdf#ETA203). Basically, the data covers workers are who are not currently working, but are actively looking for jobs, and are receiving UI benefits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Content\n",
    "<a id = \"toc\"> </a>\n",
    "\n",
    "Click on the links below to go to each section.\n",
    "\n",
    "- [1. Preparation Work](#prep) \\\n",
    "    **Key functions**: `import ... as ...`, generate a string variable\n",
    "    \n",
    "    \n",
    "- [2. Import Data](#import_data) \\\n",
    "    **Key functions**: `pd.read_csv()`, `.head()`, `.tail()`\n",
    "    \n",
    "    \n",
    "- [3. Basic Data Exploration Functions](#data_expl) \\\n",
    "    **Key functions**: `.shape`, `.columns`, `decribe()`, `info()`, `.unique()`, `.nunique()`\n",
    "\n",
    "\n",
    "- [4. Basic DataFrame Operations](#data_oper)\n",
    "    - [4.1. Slice a DataFrame](#slice_data) \\\n",
    "    **Key functions**: DataFrame slicing, `pd.to_datetime()`\n",
    "    - [4.2. Generate New Columns](#new_col) \\\n",
    "    **Key functions**: `round()`\n",
    "    \n",
    "    \n",
    "- [5. Simple Visualization](#viz) \\\n",
    "    **Key functions**: `plt.plot()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparation Work\n",
    "\n",
    "While Python has basic arithmetic and data operation functions, they are not sufficient for us to do data analysis. Similar to other data analysis software you may be already familiar with (such as R), our first step is to import libaries (or packages). The two most commonly used data manipulation libaries in Python are [Pandas](https://pandas.pydata.org/docs/user_guide/10min.html) and [Numpy](https://numpy.org/doc/stable/user/). We typically use these two libraries together. To import them, we just need to run `import pandas as pd` and `import numpy as np`. These two lines of code import the libraries and also allow us to use abbreviations `pd` and `np` in our code later to refer to Pandas and Numpy. <font color='red'>*You need to import libraries again whenever you start/reopen a Jupyter Notebook.*</font>\n",
    "\n",
    "In addition, we will use the [Matplotlib](https://matplotlib.org/stable/users/explain/quick_start.html) library to create simple visualizations in the last section of this notebook. Here, we use `import matplotlib.pyplot as plt` to import the library and use the abbreviation `plt` to refer to it in the code.\n",
    "\n",
    "> **It is a good practice to comment your code**. This way, it is easier for you to remember what each line/section of code means and also easier for others to read your code. In Python, when you add `#` to a line in the code cell, Python will treat it as a comment and will not executive that line of code. For example, in the code below, `#Data analysis libraries` is my comment (i.e., note) to the code for importing Pandas and Numpy. \n",
    "\n",
    "> **If this is the first time you use Jupyter Notebook and Python on your laptop**, you need to install Pandas, Numpy, and Matplotlib first. Just remove the `#` in front of `pip install pandas numpy matplotlib` and run the code. <font color='red'>You only need to install these libraries once on your laptop.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pandas numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data analysis libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Data visualization library\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to define file directories. This step not only makes our code aesthetically pleasing and easy to read (so that you do not have long strings in your code later), but also allows us to easily change directories (so that you do not need to search everywhere in your code to make changes). In the code below, I define my data directory in a **string** variable `data_directory`. <font color='red'> **Before you run the code below, please change the string to your own directory (the place where you put the data on your laptop).** </font> Note that you need to use the forward slash `/` to define your directory. You can also define other directories, such as where you want to put your results.\n",
    "\n",
    "> In Python, a **string** variable is a data type used to store and manipulate sequences of characters, such as text, within a program. It is usually surrounded by single quotation marks (such as `'a'`), or double quotation marks (such as `\"a\"`). Check [this tutorial](https://www.w3schools.com/python/python_strings.asp) if you would like to learn more about the string variable in Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define data directories\n",
    "#You need to change it to your own data directory\n",
    "data_directory = 'YOUR DIRECTORY'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Data\n",
    "<a id = \"import_data\"> </a>\n",
    "\n",
    "[Go back to Table of Content](#toc)\n",
    "\n",
    "Now we can import the ETA203 report data. Since it is a csv file, we need to use `pd.read_csv()` to read in the data. (Recall that `pd` refers to the Pandas library.) Inside of the parenthesis, we need to use a string to define the directory and the data file name. Note that in the previous section, we have already defined the data directory in the string variable `data_directory`. Here, we just need to use `+` to combine the string in `data_directory` and the data file name string `'eta203_insured_unemployed.csv'`.\n",
    "\n",
    "We save the data to a DataFrame object `eta203_df`. \"`_df`\" is a naming convention for DataFrame objects. This way, it is easy for us to keep track and remember the objects defined in a notebook. To check what is in this DataFrame, you can just type `eta203_df` in a code cell. However, we usually do not need to see the complete dataset. We can just add `.head()` after a DataFrame object to check the first five rows of a DataFrame. If you want to see more/fewer rows, you can specify the number of rows inside of the parenthesis, such as `.head(10)`. Of course, you can also check the last five rows of a DataFrame by using `.tail()`.\n",
    "\n",
    "We can see that the first five rows show Alaska's insured unemployed data between August and December 1994. The data is updated at the end of each month. You may also notice the three dots \"...\" between columns `c15` and `c84`. This is because we have many columns in this dataset and Python does not show every column. Later, you will learn how to check what columns are in a DataFrame and how to subset a DataFrame so that you only keep the columns you need. \n",
    "\n",
    "> In Python, you can \"add\" two strings by using `+`. The `+` operation will concatenate the two strings. You can remove the `#` in front of the `print()` code to see how the results differ when you add two strings vs. when you add two numbers.\n",
    "\n",
    "> You may get a warning \"*DtypeWarning: Columns (15) have mixed types*\". Let's ignore it for now. We will deal with it in the later notebooks.\n",
    "\n",
    "You can import data in other file types in Python. In the later notebooks, you will have chances to use the functions below:\n",
    "- `pd.read_excel()`: imports data in Excel files. See the document [here](https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html).\n",
    "- `pd.read_stata()`: imports STATA data files. See the document [here](https://pandas.pydata.org/docs/reference/api/pandas.read_stata.html#pandas.read_stata)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment the code below to see our data directory string\n",
    "#print(data_directory + 'eta203_insured_unemployed.csv')\n",
    "\n",
    "#Uncomment the code below to see the difference between string operation and arithmetic operation in Python\n",
    "#print('2' + '2')\n",
    "#print(2 + 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import ETA 203 data\n",
    "eta203_df = pd.read_csv(data_directory + 'eta203_insured_unemployed.csv')\n",
    "\n",
    "#Check your data.\n",
    "#The .head() function allows you to see the first five rows of your data\n",
    "#If you want to include more rows, you can specify the number in the parenthesis, such as .head(10)\n",
    "eta203_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The .tail() function allows you to see the last five rows of your data\n",
    "#If you want to include more rows, you can specify the number in the parenthesis, such as .tail(10)\n",
    "eta203_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Data Exploration Functions\n",
    "<a id = \"data_expl\"> </a>\n",
    "\n",
    "[Go back to Table of Content](#toc)\n",
    "\n",
    "In this section, we will check some basic information about our DataFrame `eta203_df`, such as:\n",
    "- how many rows and how many columns `eta203_df` contains;\n",
    "- what information `eta203_df` holds;\n",
    "- the data type of each column (string, integer, float, etc.);\n",
    "- the distributions of numeric columns;\n",
    "- the unique values and the number of unique values of key columns, such as `st` (state).\n",
    "\n",
    "This step helps you gain a general understanding of the data, ensures that you are working with the correct dataset, and allows you to plan for further data cleaning. To check our DataFrame, we can use the following functions:\n",
    "\n",
    "a). **`.shape`**: The `.shape` function tells you the number of rows and the number of columns in a DataFrame. We can see that our data has 19,845 rows and 69 columns.\n",
    "\n",
    "b). **`.columns`**: The `.columns` function gives you the full list of columns in a DataFrame. This is one of the ways for us to check all the columns when our DataFrame has many columns. Recall that we cannot see all of the columns by just running `eta203_df.head()`.\n",
    "\n",
    "c). **`.info()`**: The `.info()` function does not only show you all the columns in a DataFrame, but also tells you the number of not-null (i.e., not missing) rows and the type of a column. For example, we can see that the column `st` has 19,845 not-null rows and is an object column. This implies that there is no missing values in `st` and all the values in `st` are strings. The other column type in our DataFrame is int64 (i.e., integer). \n",
    "\n",
    "d). **`.describe()`**: The `.describe()` function shows you the distributions of values in numeric columns, including count, mean, standard deviation, minimum, maximum, median, and 25 and 75 percentile. We can see that only columns of type int64 are present in the results. Also, the minimum values of some columns are zero. They could be true zeros (meaning in some months and some states, the number of male/female/... insured unemployed workers are zero) or missing values. We should further examine and clean these values later. In some cases (such as when examining wage data), you also want to pay attention to the maximum values and consider to remove outliers.\n",
    "\n",
    "e). **`.unique()`** and **`.nunique()`**: The **`.unique()`** function checks the unique values in a column and the **`.nunique()`** function tells you how many unique values are in that column. In the example below, we can see that the values in column `st` are abbreviations of states. They are all capital letters. This is important for us to do basic DataFrame operations based on conditions later. In addition, there are 53 unique values in `st`. \n",
    "\n",
    "> **Two ways to refer to a column**: You may have noticed that when referring to the column `st`, we used two different ways: `eta203_df['st']` and `eta203_df.st`. In Python, you can use the square brackets and a column name (must be in a string) to refer to a column. We will see more examples of how to refer to multiple columns later. If you only need one column, you can also just use the dot and column name method (you do not need the quotation marks in this case). Note that the second method will not work if your column name has space in it.\n",
    "\n",
    "> **To use parenthesis or not to use parenthesis**: You may also have noticed that some of the functions have parentheses, such as `.head()` and `.unique()`, while others do not have parentheses, such as `.shape`. The presence of parentheses depend on whether a function requires arguments or not. For example, as you have seen before, you can specify the number of rows you want to see with `.head(10)` function. For another example, you can tell the `.describe()` function what percentiles you want to examine. You can try `eta203_df.describe(percentiles = [0.05, 0.95])` and see how the results differ. On the other hand, if a function just tells you the attributes or properties of a DataFrame, you do not need to add the parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check number of rows and number of columns\n",
    "eta203_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the names of the DataFrame's columns\n",
    "eta203_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Check basic information of a DataFrame\n",
    "eta203_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Basic descriptive statistics of all numeric columns\n",
    "eta203_df.describe().astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unique values in a column\n",
    "eta203_df['st'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta203_df['st']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta203_df[['st', 'rptdate']].head().columns[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta203_df.st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of unique values in a column\n",
    "eta203_df.st.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Basic DataFrame Operations\n",
    "<a id = \"data_oper\"> </a>\n",
    "\n",
    "### 4.1. Slice a DataFrame\n",
    "<a id = \"slice_data\"> </a>\n",
    "\n",
    "[Go back to Table of Content](#toc)\n",
    "\n",
    "As mentioned in the previous section, our dataset contains many columns (number of insured unemployed by gender, race, age, and industry) and many rows (number of insured unemployed by state from August 1994 to November 2023). When conducting data analysis, we do not always need all the columns and rows in a DataFrame. Sometimes, we just need a few columns or just need to make changes to a subset of data. In this case, we need to understand an important operation in Python: **slicing a DataFrame**. It is essential for extracting specific information from a large dataset, allowing for focused analysis and exploration. This capability enables us to isolate relevant columns or rows based on specific criteria, enhancing the efficiency of data manipulation and interpretation. \n",
    "\n",
    "**Assume we want to understand the trends in insured unemployed workers in Ohio by gender between 2013 and 2022**. To help you understand data slicing techniques in Python, let's use three steps to get the data we need.\n",
    "\n",
    "**Step 1: Limit the data to Ohio data** \\\n",
    "In this step, we use the code `oh_eta203_df = eta203_df[eta203_df['st'] == 'OH']`. Inside of the bracket, we specify a condition `eta203_df['st'] == 'OH'`, which tells Python to examine whether the value of `st` in a row is `OH`. By including this condition inside of the bracket after a DataFrame (i.e., `eta203_df[eta203_df['st'] == 'OH']`), we ask Python to filter the rows based on this condition. Python only keeps the rows where the condition is true. Then we assign the resulting rows to a new DataFrame `oh_eta203_df`. We can check the new DataFrame by using `oh_eta203_df.head()`. We can see that the states of the first five rows are `OH` now, as opposed to `AK`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: limit to Ohio data\n",
    "oh_eta203_df = eta203_df[eta203_df['st'] == 'OH']\n",
    "\n",
    "#oh_eta203_df = eta203_df[eta203_df['st'].isin(['OH', 'NY'])]\n",
    "\n",
    "#Check the data\n",
    "oh_eta203_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Limit the timeframe to 2013-2022** \\\n",
    "The second column `rptdate` shows the reporting period of the data in each row. Based on our investigation in the previous section, `rptdate` is an object column, i.e., all the values in this column are strings. While we can recognize these values as dates, Python treats them as text. It would be hard for us to filter data by using these dates in string type. Luckily, Pandas can easily convert these strings into date format. Then we can use two simple conditions to filter the data. \n",
    "\n",
    "First, we use `pd.to_datetime()` function to convert `oh_eta203_df['rptdate']` to  datetime format. Inside of this function, we need to specify the column we want to convert and the format of the date (month/day/year). Then we assign the resulting data to the `rptdate` column. (Of course, you can assign the resulting data to a new column.) The datetime format allows us to conveniently perform analysis with dates. We will dive deeper into it in later notebooks. If you are interested, you can read the [document](https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html) for `pd.to_datetime()`.\n",
    "\n",
    "Second, we use conditions to filter the data again and save the resulting data to a new DataFrame `oh_eta203_1322_df`. This operation is similar to step 1, except that now we specify two conditions and connect them with `&`. The conditions `(oh_eta203_df['rptdate'] >= '2013-01-01') & (oh_eta203_df['rptdate'] <= '2022-12-31')` tell Python that we want rows in `oh_eta203_df` where `rptdate` is between January 1, 2013, and December 31, 2022. Note that since we have converted `rptdate` to datetime format now, we can specify the conditions as if we are using numbers (e.g., x > 10). You can use strings in the 'YYYY-MM-DD' format to refer to dates in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Limit to 2013-2022 Ohio data\n",
    "\n",
    "#Change rptdate column to datetime format\n",
    "oh_eta203_df['rptdate'] = pd.to_datetime(oh_eta203_df['rptdate'], format = '%m/%d/%Y')\n",
    "\n",
    "#Limit the data to 2013-2022\n",
    "oh_eta203_1322_df = oh_eta203_df[(oh_eta203_df['rptdate'] >= '2013-01-01') & (oh_eta203_df['rptdate'] <= '2022-12-31')]\n",
    "\n",
    "#Check the data\n",
    "oh_eta203_1322_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Limit the columns to reporting period, female, male, and gender missing counts (`rptdate`, `c2`, `c3`, and `c4`)**:\n",
    "Now, the DataFrame `oh_eta203_1322_df` only contains Ohio data from 2013 to 2022. You have already learned how to refer to one column of a DataFrame: just add two square brackets after the DataFrame name and put the column name as a string inside of the brackets (such as `oh_eta203_1322_df['rptdate']`). To get multiple columns, we need to specify a list of column names `['rptdate', 'c2', 'c3', 'c4']` and put them inside of the brackets after the DataFrame name `oh_eta203_1322_df[['rptdate', 'c2', 'c3', 'c4']]`. Note that each column name still needs to be a string and, **yes, there are two pairs of brackets!** Please read [this document](https://www.w3schools.com/python/python_lists.asp) to learn more about list in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3: Only keep the columns we need\n",
    "#We save the resulting data to the same DataFrame `oh_eta203_1322_df`. Of course, you can create a new DataFrame.\n",
    "oh_eta203_1322_df = oh_eta203_1322_df[['rptdate', 'c2', 'c3', 'c4']]\n",
    "\n",
    "#Check the data\n",
    "oh_eta203_1322_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Generate New Columns\n",
    "<a id = \"new_col\"> </a>\n",
    "\n",
    "[Go back to Table of Content](#toc)\n",
    "\n",
    "**Assume we want to calculate the percentages of male and female insured unemployed workers**. Note that our data does not have total number of insured unemployed workers in a month. To calculate the total number, we can use `+` to sum up all the counts in `c2`, `c3`, and `c4`. We can save the resulting data to a new column, `total`.\n",
    "\n",
    "To calculate percentage of male insured unemployed workers, we can use `/` to devide the counts in `oh_eta203_1322_df['c2']` by the total counts in `oh_eta203_1322_df['total']`. The function `round()` allows us to choose the number of digits we want to keep after the decimal. Here, we keep three digits so that it is easy for us to read the results. We can see that during the first five months of 2013, the percentage of male insured unemployed workers varied between 60% and 70%, while females varied between 28% and 39%. \n",
    "\n",
    "What about during the pandemic period? Are there any unusal trends? In this case, we can use the slicing technique from the previous section to limit the data between January and May 2020. We can see that the first three months of 2020 has the same trend as 2013. However, immediately after the start of the pandemic, the percentage of insured unemployed females exceeded the percentage of insured unemployed males."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate total number of insured unemployed in a month\n",
    "oh_eta203_1322_df['total'] = oh_eta203_1322_df['c2'] + oh_eta203_1322_df['c3'] + oh_eta203_1322_df['c4']\n",
    "\n",
    "#Check the data\n",
    "oh_eta203_1322_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate percentages\n",
    "oh_eta203_1322_df['male_pct'] = round(oh_eta203_1322_df['c2'] / oh_eta203_1322_df['total'], 3)\n",
    "oh_eta203_1322_df['female_pct'] = round(oh_eta203_1322_df['c3'] / oh_eta203_1322_df['total'], 3)\n",
    "oh_eta203_1322_df['gender_missing_pct'] = round(oh_eta203_1322_df['c4'] / oh_eta203_1322_df['total'], 3)\n",
    "\n",
    "#Check the data\n",
    "oh_eta203_1322_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Percentages of male and female insured unemployed workers during the pandemic period\n",
    "oh_eta203_1322_df[(oh_eta203_1322_df['rptdate'] >= '2020-01-31') & (oh_eta203_1322_df['rptdate'] <= '2020-05-31')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Simple Visualization\n",
    "<a id = \"viz\"> </a>\n",
    "\n",
    "[Go back to Table of Content](#toc)\n",
    "\n",
    "By eyeballing the data, we can examine a limited number of rows each time. You may wonder: is it common that there is always a higher number of males than females among insured unemployed workers? How unusual is the trend immediately after the pandemic? To explore the general trends and patterns in a large dataset, we can use some simple visualizations. We will dedicate more time to visualizations in later classes. Here, we use a line plot to show the number of male insured unemployed workers in Ohio over time.\n",
    "\n",
    "Recall that `plt` is the abbrevation of the visualization library `matplotlib.pyplot`. To begin with, we should define the size of the graph so that it is easy for us to observe trends. We can specify the size of the plot with code `plt.figure(figsize = (10, 6))`. The first number is the width and the second number is the height of the graph. To create a line chart, we use `plt.plot()`. Inside of this function, we need to define at least two arguments: 1) the x-axis values, `oh_eta203_1322_df['rptdate']`; and 2) the y-axis values, `oh_eta203_1322_df['c2']`. Of course, you can customize many aspects of the graph with Matplotlib functions. We will discuss the visualizations in more details in later notebooks.\n",
    "\n",
    "Then we use `plt.show()` to display the line chart in Jupyter Notebook. We can see that there is a spike in the number of male insured unemployed workers during the pandemic. The high number lasted for over a year. Another pattern is that the number of male insured unemployed workers fluctuate over time, with the highest points being the winter of a year. This fluctuation is usually referred to as seasonality. Here is an interesting [paper about seasonality](https://www.chicagofed.org/publications/economic-perspectives/2018/3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a simple line chart for counts of male insured unemployed workers\n",
    "\n",
    "#Define the size of the plot\n",
    "plt.figure(figsize = (10, 6))\n",
    "\n",
    "#Plot the line chart.\n",
    "#Here, we define x axis to be the reporting period and y axis to be the counts of male insured unemployed workers\n",
    "plt.plot(oh_eta203_1322_df['rptdate'], oh_eta203_1322_df['c2'])\n",
    "\n",
    "#Show the plot in the notebook\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add a line plot for females on the same graph, we just need to repeat the `plt.plot()` code and replace the y-axis values with `oh_eta203_1322_df['c3']`. One additional parameter we specify here is `label = `, which helps us distinguish the lines for males and females. Moreover, we use `plt.legend()` to show the legend. \n",
    "\n",
    "We can see that during the majority of the time, there are fewer female insured unemployed workers than male insured unemployed workers, especially during the winter. However, during the middle of each year (summer time), the gender ratio is relatively more balanced. At the start of the pandemic, female counts exceeded male counts, but the number of females declined faster than male counts.\n",
    "\n",
    "> **What's next? How would these general trends inform your project scope/research questions?** After exploring the general trends in your dataset, you may delve further into one or more aspects. For example, why do male insured unemployed workers exhibit stronger seasonality than females? Could it be because of the industries they work in? Could it be related to changes in demographics in recently years (e.g., aging population)? Is this trend specific to Ohio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use a simple line chart to compare counts of male and female insured unemployed workers over time\n",
    "\n",
    "#Define the size of the plot\n",
    "plt.figure(figsize = (10, 6))\n",
    "\n",
    "#Plot the line chart.\n",
    "#Here, we define x axis to be the reporting period and y axis to be the counts of male insured unemployed workers\n",
    "plt.plot(oh_eta203_1322_df['rptdate'], oh_eta203_1322_df['c2'], label = 'Male')\n",
    "plt.plot(oh_eta203_1322_df['rptdate'], oh_eta203_1322_df['c3'], label = 'Female')\n",
    "\n",
    "#Show legend\n",
    "plt.legend()\n",
    "\n",
    "#Show the plot in the notebook\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations! You have just completed the first notebook for the Public Sector Data Sciences and Management class. In the subsequent notebooks, you will learn more data analysis techniques. Now, please spend some time to review the basic functions you learned in this notebook and consider how they may assist you with your project.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
